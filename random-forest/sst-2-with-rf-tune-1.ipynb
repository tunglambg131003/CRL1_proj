{
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "V28"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.13",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "accelerator": "TPU",
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "sourceId": 320111,
     "sourceType": "datasetVersion",
     "datasetId": 134715
    },
    {
     "sourceId": 8535935,
     "sourceType": "datasetVersion",
     "datasetId": 5098610
    }
   ],
   "dockerImageVersionId": 30699,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook",
   "isGpuEnabled": true
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "import torchtext\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from torchtext.vocab import Vectors\n",
    "from tqdm import tqdm_notebook\n",
    "import pandas as pd"
   ],
   "metadata": {
    "id": "Fi_TgDni-qkU",
    "execution": {
     "iopub.status.busy": "2024-05-29T15:14:44.697725Z",
     "iopub.execute_input": "2024-05-29T15:14:44.697993Z",
     "iopub.status.idle": "2024-05-29T15:14:51.303053Z",
     "shell.execute_reply.started": "2024-05-29T15:14:44.697969Z",
     "shell.execute_reply": "2024-05-29T15:14:51.302094Z"
    },
    "trusted": true
   },
   "execution_count": 1,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "train_df = pd.read_csv(\"/kaggle/input/sst-2-dataset/train.csv\")\n",
    "validation_df = pd.read_csv(\"/kaggle/input/sst-2-dataset/validation.csv\")\n",
    "\n",
    "train_df.drop(columns = \"idx\")\n",
    "validation_df.drop(columns = \"idx\")\n",
    "\n",
    "\n",
    "test_df = pd.read_csv(\"/kaggle/input/imdb-dataset-of-50k-movie-reviews/IMDB Dataset.csv\")\n",
    "test_df.rename(columns={'review': 'sentence', 'sentiment': 'label'}, inplace=True)\n",
    "test_df['label'] = test_df['label'].map({'negative': 0, 'positive': 1})\n",
    "output_path = \"/kaggle/working/modified_imdb_dataset.csv\"\n",
    "test_df.to_csv(output_path, index=False)"
   ],
   "metadata": {
    "id": "Rj08deiNAos2",
    "execution": {
     "iopub.status.busy": "2024-05-29T15:14:51.304715Z",
     "iopub.execute_input": "2024-05-29T15:14:51.305183Z",
     "iopub.status.idle": "2024-05-29T15:14:55.420850Z",
     "shell.execute_reply.started": "2024-05-29T15:14:51.305156Z",
     "shell.execute_reply": "2024-05-29T15:14:55.420073Z"
    },
    "trusted": true
   },
   "execution_count": 2,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "train_df[\"sentence\"] = train_df[\"sentence\"].str.lower()\n",
    "validation_df[\"sentence\"] = validation_df[\"sentence\"].str.lower()\n",
    "test_df[\"sentence\"] = test_df[\"sentence\"].str.lower()"
   ],
   "metadata": {
    "id": "74XaPx7JBg70",
    "execution": {
     "iopub.status.busy": "2024-05-29T15:14:55.421826Z",
     "iopub.execute_input": "2024-05-29T15:14:55.422098Z",
     "iopub.status.idle": "2024-05-29T15:14:55.625640Z",
     "shell.execute_reply.started": "2024-05-29T15:14:55.422075Z",
     "shell.execute_reply": "2024-05-29T15:14:55.624852Z"
    },
    "trusted": true
   },
   "execution_count": 3,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import string\n",
    "PUNCT_TO_REMOVE = string.punctuation\n",
    "def remove_punctuation(text):\n",
    "    \"\"\"custom function to remove the punctuation\"\"\"\n",
    "    return text.translate(str.maketrans('', '', PUNCT_TO_REMOVE))\n",
    "\n",
    "train_df[\"text_wo_punct\"] = train_df[\"sentence\"].apply(lambda text: remove_punctuation(text))\n",
    "validation_df[\"text_wo_punct\"] = validation_df[\"sentence\"].apply(lambda text: remove_punctuation(text))\n",
    "test_df[\"text_wo_punct\"] = test_df[\"sentence\"].apply(lambda text: remove_punctuation(text))"
   ],
   "metadata": {
    "id": "JoXyBr3QBz5X",
    "execution": {
     "iopub.status.busy": "2024-05-29T15:14:55.627575Z",
     "iopub.execute_input": "2024-05-29T15:14:55.627873Z",
     "iopub.status.idle": "2024-05-29T15:14:57.078463Z",
     "shell.execute_reply.started": "2024-05-29T15:14:55.627849Z",
     "shell.execute_reply": "2024-05-29T15:14:57.077571Z"
    },
    "trusted": true
   },
   "execution_count": 4,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import nltk\n",
    "\n",
    "# Ensure you have downloaded the stopwords dataset\n",
    "nltk.download('stopwords')\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1sIuZyKjCojq",
    "outputId": "4f3b7c8e-b129-42c0-85f0-c24316f37f3b",
    "execution": {
     "iopub.status.busy": "2024-05-29T15:14:57.079563Z",
     "iopub.execute_input": "2024-05-29T15:14:57.079903Z",
     "iopub.status.idle": "2024-05-29T15:14:58.833819Z",
     "shell.execute_reply.started": "2024-05-29T15:14:57.079872Z",
     "shell.execute_reply": "2024-05-29T15:14:58.832932Z"
    },
    "trusted": true
   },
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "text": "[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n",
     "output_type": "stream"
    },
    {
     "execution_count": 5,
     "output_type": "execute_result",
     "data": {
      "text/plain": "True"
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "\", \".join(stopwords.words('english'))\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "id": "qvAoSqdQDH1M",
    "outputId": "7cf7361a-32fb-4054-9f7d-d6e7cd88142c",
    "execution": {
     "iopub.status.busy": "2024-05-29T15:14:58.834873Z",
     "iopub.execute_input": "2024-05-29T15:14:58.835171Z",
     "iopub.status.idle": "2024-05-29T15:14:58.847298Z",
     "shell.execute_reply.started": "2024-05-29T15:14:58.835143Z",
     "shell.execute_reply": "2024-05-29T15:14:58.846434Z"
    },
    "trusted": true
   },
   "execution_count": 6,
   "outputs": [
    {
     "execution_count": 6,
     "output_type": "execute_result",
     "data": {
      "text/plain": "\"i, me, my, myself, we, our, ours, ourselves, you, you're, you've, you'll, you'd, your, yours, yourself, yourselves, he, him, his, himself, she, she's, her, hers, herself, it, it's, its, itself, they, them, their, theirs, themselves, what, which, who, whom, this, that, that'll, these, those, am, is, are, was, were, be, been, being, have, has, had, having, do, does, did, doing, a, an, the, and, but, if, or, because, as, until, while, of, at, by, for, with, about, against, between, into, through, during, before, after, above, below, to, from, up, down, in, out, on, off, over, under, again, further, then, once, here, there, when, where, why, how, all, any, both, each, few, more, most, other, some, such, no, nor, not, only, own, same, so, than, too, very, s, t, can, will, just, don, don't, should, should've, now, d, ll, m, o, re, ve, y, ain, aren, aren't, couldn, couldn't, didn, didn't, doesn, doesn't, hadn, hadn't, hasn, hasn't, haven, haven't, isn, isn't, ma, mightn, mightn't, mustn, mustn't, needn, needn't, shan, shan't, shouldn, shouldn't, wasn, wasn't, weren, weren't, won, won't, wouldn, wouldn't\""
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "STOPWORDS = set(stopwords.words('english'))\n",
    "def remove_stopwords(text):\n",
    "    \"\"\"custom function to remove the stopwords\"\"\"\n",
    "    return \" \".join([word for word in str(text).split() if word not in STOPWORDS])\n",
    "\n",
    "train_df[\"text_wo_stop\"] = train_df[\"text_wo_punct\"].apply(lambda text: remove_stopwords(text))\n",
    "validation_df[\"text_wo_stop\"] = validation_df[\"text_wo_punct\"].apply(lambda text: remove_stopwords(text))\n",
    "test_df[\"text_wo_stop\"] = test_df[\"text_wo_punct\"].apply(lambda text: remove_stopwords(text))"
   ],
   "metadata": {
    "id": "L6eBYH1ODNoi",
    "execution": {
     "iopub.status.busy": "2024-05-29T15:14:58.848451Z",
     "iopub.execute_input": "2024-05-29T15:14:58.848734Z",
     "iopub.status.idle": "2024-05-29T15:15:01.238087Z",
     "shell.execute_reply.started": "2024-05-29T15:14:58.848708Z",
     "shell.execute_reply": "2024-05-29T15:15:01.237172Z"
    },
    "trusted": true
   },
   "execution_count": 7,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "def stem_words(text):\n",
    "    return \" \".join([stemmer.stem(word) for word in text.split()])\n",
    "\n",
    "train_df[\"text_stemmed\"] = train_df[\"text_wo_stop\"].apply(lambda text: stem_words(text))\n",
    "validation_df[\"text_stemmed\"] = validation_df[\"text_wo_stop\"].apply(lambda text: stem_words(text))\n",
    "test_df[\"text_stemmed\"] = test_df[\"text_wo_stop\"].apply(lambda text: stem_words(text))"
   ],
   "metadata": {
    "id": "_u7SvEcCDgNE",
    "execution": {
     "iopub.status.busy": "2024-05-29T15:15:01.239329Z",
     "iopub.execute_input": "2024-05-29T15:15:01.239633Z",
     "iopub.status.idle": "2024-05-29T15:18:34.167785Z",
     "shell.execute_reply.started": "2024-05-29T15:15:01.239604Z",
     "shell.execute_reply": "2024-05-29T15:18:34.166935Z"
    },
    "trusted": true
   },
   "execution_count": 8,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "\n",
    "# Tokenize sentences\n",
    "nltk.download('punkt')\n",
    "train_df['tokens'] = train_df['text_stemmed'].apply(lambda x: nltk.word_tokenize(x))\n",
    "\n",
    "# Create a word index (manual embedding)\n",
    "word_set = set()\n",
    "for tokens in train_df['tokens']:\n",
    "    word_set.update(tokens)\n",
    "\n",
    "word_index = {word: i+1 for i, word in enumerate(word_set)}  # Start indexing from 1\n",
    "\n",
    "# Convert tokens to integers\n",
    "train_df['indexed_tokens'] = train_df['tokens'].apply(lambda x: [word_index[token] for token in x])\n",
    "\n",
    "# Optional: Pad sequences to ensure uniform length\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "max_length = 100  # Define maximum length of sequences\n",
    "train_df['padded_tokens'] = pad_sequences(train_df['indexed_tokens'], maxlen=max_length, padding='post').tolist()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "621ta8LXD_r_",
    "outputId": "470f5119-f5d5-49ef-be45-ff019b120290",
    "execution": {
     "iopub.status.busy": "2024-05-29T15:18:34.168889Z",
     "iopub.execute_input": "2024-05-29T15:18:34.169165Z",
     "iopub.status.idle": "2024-05-29T15:18:56.288933Z",
     "shell.execute_reply.started": "2024-05-29T15:18:34.169136Z",
     "shell.execute_reply": "2024-05-29T15:18:56.288032Z"
    },
    "trusted": true
   },
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "text": "[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "2024-05-29 15:18:45.125166: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-05-29 15:18:45.125277: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-05-29 15:18:45.269495: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "train_df"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 631
    },
    "id": "ars32olrGFtI",
    "outputId": "a39d96ff-5754-4026-e8b0-44df3bc69b2e",
    "execution": {
     "iopub.status.busy": "2024-05-29T15:18:56.291704Z",
     "iopub.execute_input": "2024-05-29T15:18:56.292276Z",
     "iopub.status.idle": "2024-05-29T15:18:56.345249Z",
     "shell.execute_reply.started": "2024-05-29T15:18:56.292249Z",
     "shell.execute_reply": "2024-05-29T15:18:56.344143Z"
    },
    "trusted": true
   },
   "execution_count": 10,
   "outputs": [
    {
     "execution_count": 10,
     "output_type": "execute_result",
     "data": {
      "text/plain": "         idx                                           sentence  label  \\\n0          0       hide new secretions from the parental units       0   \n1          1               contains no wit , only labored gags       0   \n2          2  that loves its characters and communicates som...      1   \n3          3  remains utterly satisfied to remain the same t...      0   \n4          4  on the worst revenge-of-the-nerds clichés the ...      0   \n...      ...                                                ...    ...   \n67344  67344                               a delightful comedy       1   \n67345  67345                   anguish , anger and frustration       0   \n67346  67346  at achieving the modest , crowd-pleasing goals...      1   \n67347  67347                                  a patient viewer       1   \n67348  67348  this new jangle of noise , mayhem and stupidit...      0   \n\n                                           text_wo_punct  \\\n0           hide new secretions from the parental units    \n1                    contains no wit  only labored gags    \n2      that loves its characters and communicates som...   \n3      remains utterly satisfied to remain the same t...   \n4      on the worst revengeofthenerds clichés the fil...   \n...                                                  ...   \n67344                               a delightful comedy    \n67345                    anguish  anger and frustration    \n67346  at achieving the modest  crowdpleasing goals i...   \n67347                                  a patient viewer    \n67348  this new jangle of noise  mayhem and stupidity...   \n\n                                            text_wo_stop  \\\n0                     hide new secretions parental units   \n1                              contains wit labored gags   \n2      loves characters communicates something rather...   \n3            remains utterly satisfied remain throughout   \n4      worst revengeofthenerds clichés filmmakers cou...   \n...                                                  ...   \n67344                                  delightful comedy   \n67345                          anguish anger frustration   \n67346          achieving modest crowdpleasing goals sets   \n67347                                     patient viewer   \n67348  new jangle noise mayhem stupidity must serious...   \n\n                                            text_stemmed  \\\n0                            hide new secret parent unit   \n1                                  contain wit labor gag   \n2      love charact commun someth rather beauti human...   \n3               remain utterli satisfi remain throughout   \n4      worst revengeofthenerd cliché filmmak could dredg   \n...                                                  ...   \n67344                                     delight comedi   \n67345                             anguish anger frustrat   \n67346                  achiev modest crowdpleas goal set   \n67347                                     patient viewer   \n67348  new jangl nois mayhem stupid must seriou conte...   \n\n                                                  tokens  \\\n0                      [hide, new, secret, parent, unit]   \n1                             [contain, wit, labor, gag]   \n2      [love, charact, commun, someth, rather, beauti...   \n3         [remain, utterli, satisfi, remain, throughout]   \n4      [worst, revengeofthenerd, cliché, filmmak, cou...   \n...                                                  ...   \n67344                                  [delight, comedi]   \n67345                         [anguish, anger, frustrat]   \n67346            [achiev, modest, crowdpleas, goal, set]   \n67347                                  [patient, viewer]   \n67348  [new, jangl, nois, mayhem, stupid, must, serio...   \n\n                                          indexed_tokens  \\\n0                         [1468, 9934, 2775, 1438, 2739]   \n1                               [1113, 10676, 422, 2198]   \n2       [1681, 6313, 7531, 4957, 2439, 1141, 2217, 2471]   \n3                         [6225, 5358, 7088, 6225, 9407]   \n4                  [3647, 5690, 8997, 1136, 10290, 7921]   \n...                                                  ...   \n67344                                       [7309, 8380]   \n67345                                 [3588, 7222, 4072]   \n67346                     [9720, 3480, 9079, 1085, 6819]   \n67347                                        [206, 4606]   \n67348  [9934, 8092, 10602, 2383, 8331, 4289, 8166, 38...   \n\n                                           padded_tokens  \n0      [1468, 9934, 2775, 1438, 2739, 0, 0, 0, 0, 0, ...  \n1      [1113, 10676, 422, 2198, 0, 0, 0, 0, 0, 0, 0, ...  \n2      [1681, 6313, 7531, 4957, 2439, 1141, 2217, 247...  \n3      [6225, 5358, 7088, 6225, 9407, 0, 0, 0, 0, 0, ...  \n4      [3647, 5690, 8997, 1136, 10290, 7921, 0, 0, 0,...  \n...                                                  ...  \n67344  [7309, 8380, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n67345  [3588, 7222, 4072, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n67346  [9720, 3480, 9079, 1085, 6819, 0, 0, 0, 0, 0, ...  \n67347  [206, 4606, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...  \n67348  [9934, 8092, 10602, 2383, 8331, 4289, 8166, 38...  \n\n[67349 rows x 9 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>idx</th>\n      <th>sentence</th>\n      <th>label</th>\n      <th>text_wo_punct</th>\n      <th>text_wo_stop</th>\n      <th>text_stemmed</th>\n      <th>tokens</th>\n      <th>indexed_tokens</th>\n      <th>padded_tokens</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>hide new secretions from the parental units</td>\n      <td>0</td>\n      <td>hide new secretions from the parental units</td>\n      <td>hide new secretions parental units</td>\n      <td>hide new secret parent unit</td>\n      <td>[hide, new, secret, parent, unit]</td>\n      <td>[1468, 9934, 2775, 1438, 2739]</td>\n      <td>[1468, 9934, 2775, 1438, 2739, 0, 0, 0, 0, 0, ...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>contains no wit , only labored gags</td>\n      <td>0</td>\n      <td>contains no wit  only labored gags</td>\n      <td>contains wit labored gags</td>\n      <td>contain wit labor gag</td>\n      <td>[contain, wit, labor, gag]</td>\n      <td>[1113, 10676, 422, 2198]</td>\n      <td>[1113, 10676, 422, 2198, 0, 0, 0, 0, 0, 0, 0, ...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>that loves its characters and communicates som...</td>\n      <td>1</td>\n      <td>that loves its characters and communicates som...</td>\n      <td>loves characters communicates something rather...</td>\n      <td>love charact commun someth rather beauti human...</td>\n      <td>[love, charact, commun, someth, rather, beauti...</td>\n      <td>[1681, 6313, 7531, 4957, 2439, 1141, 2217, 2471]</td>\n      <td>[1681, 6313, 7531, 4957, 2439, 1141, 2217, 247...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>remains utterly satisfied to remain the same t...</td>\n      <td>0</td>\n      <td>remains utterly satisfied to remain the same t...</td>\n      <td>remains utterly satisfied remain throughout</td>\n      <td>remain utterli satisfi remain throughout</td>\n      <td>[remain, utterli, satisfi, remain, throughout]</td>\n      <td>[6225, 5358, 7088, 6225, 9407]</td>\n      <td>[6225, 5358, 7088, 6225, 9407, 0, 0, 0, 0, 0, ...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>on the worst revenge-of-the-nerds clichés the ...</td>\n      <td>0</td>\n      <td>on the worst revengeofthenerds clichés the fil...</td>\n      <td>worst revengeofthenerds clichés filmmakers cou...</td>\n      <td>worst revengeofthenerd cliché filmmak could dredg</td>\n      <td>[worst, revengeofthenerd, cliché, filmmak, cou...</td>\n      <td>[3647, 5690, 8997, 1136, 10290, 7921]</td>\n      <td>[3647, 5690, 8997, 1136, 10290, 7921, 0, 0, 0,...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>67344</th>\n      <td>67344</td>\n      <td>a delightful comedy</td>\n      <td>1</td>\n      <td>a delightful comedy</td>\n      <td>delightful comedy</td>\n      <td>delight comedi</td>\n      <td>[delight, comedi]</td>\n      <td>[7309, 8380]</td>\n      <td>[7309, 8380, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n    </tr>\n    <tr>\n      <th>67345</th>\n      <td>67345</td>\n      <td>anguish , anger and frustration</td>\n      <td>0</td>\n      <td>anguish  anger and frustration</td>\n      <td>anguish anger frustration</td>\n      <td>anguish anger frustrat</td>\n      <td>[anguish, anger, frustrat]</td>\n      <td>[3588, 7222, 4072]</td>\n      <td>[3588, 7222, 4072, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n    </tr>\n    <tr>\n      <th>67346</th>\n      <td>67346</td>\n      <td>at achieving the modest , crowd-pleasing goals...</td>\n      <td>1</td>\n      <td>at achieving the modest  crowdpleasing goals i...</td>\n      <td>achieving modest crowdpleasing goals sets</td>\n      <td>achiev modest crowdpleas goal set</td>\n      <td>[achiev, modest, crowdpleas, goal, set]</td>\n      <td>[9720, 3480, 9079, 1085, 6819]</td>\n      <td>[9720, 3480, 9079, 1085, 6819, 0, 0, 0, 0, 0, ...</td>\n    </tr>\n    <tr>\n      <th>67347</th>\n      <td>67347</td>\n      <td>a patient viewer</td>\n      <td>1</td>\n      <td>a patient viewer</td>\n      <td>patient viewer</td>\n      <td>patient viewer</td>\n      <td>[patient, viewer]</td>\n      <td>[206, 4606]</td>\n      <td>[206, 4606, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...</td>\n    </tr>\n    <tr>\n      <th>67348</th>\n      <td>67348</td>\n      <td>this new jangle of noise , mayhem and stupidit...</td>\n      <td>0</td>\n      <td>this new jangle of noise  mayhem and stupidity...</td>\n      <td>new jangle noise mayhem stupidity must serious...</td>\n      <td>new jangl nois mayhem stupid must seriou conte...</td>\n      <td>[new, jangl, nois, mayhem, stupid, must, serio...</td>\n      <td>[9934, 8092, 10602, 2383, 8331, 4289, 8166, 38...</td>\n      <td>[9934, 8092, 10602, 2383, 8331, 4289, 8166, 38...</td>\n    </tr>\n  </tbody>\n</table>\n<p>67349 rows × 9 columns</p>\n</div>"
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "\n",
    "# Tokenize sentences\n",
    "nltk.download('punkt')\n",
    "validation_df['tokens'] = validation_df['text_stemmed'].apply(lambda x: nltk.word_tokenize(x))\n",
    "\n",
    "# Create a word index (manual embedding)\n",
    "word_set = set()\n",
    "for tokens in validation_df['tokens']:\n",
    "    word_set.update(tokens)\n",
    "\n",
    "word_index = {word: i+1 for i, word in enumerate(word_set)}  # Start indexing from 1\n",
    "\n",
    "# Convert tokens to integers\n",
    "validation_df['indexed_tokens'] = validation_df['tokens'].apply(lambda x: [word_index[token] for token in x])\n",
    "\n",
    "# Optional: Pad sequences to ensure uniform length\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "max_length = 100  # Define maximum length of sequences\n",
    "validation_df['padded_tokens'] = pad_sequences(validation_df['indexed_tokens'], maxlen=max_length, padding='post').tolist()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "akc5FmMAG04h",
    "outputId": "b93488ef-ec8f-4b6d-ad91-e95cf26f0a62",
    "execution": {
     "iopub.status.busy": "2024-05-29T15:18:56.346554Z",
     "iopub.execute_input": "2024-05-29T15:18:56.346850Z",
     "iopub.status.idle": "2024-05-29T15:18:56.525943Z",
     "shell.execute_reply.started": "2024-05-29T15:18:56.346825Z",
     "shell.execute_reply": "2024-05-29T15:18:56.525010Z"
    },
    "trusted": true
   },
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "text": "[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "validation_df"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 649
    },
    "id": "qjG24G40HEhX",
    "outputId": "15e16022-0c98-42a7-eb16-1c86f2ea721e",
    "execution": {
     "iopub.status.busy": "2024-05-29T15:18:56.527153Z",
     "iopub.execute_input": "2024-05-29T15:18:56.527446Z",
     "iopub.status.idle": "2024-05-29T15:18:56.566154Z",
     "shell.execute_reply.started": "2024-05-29T15:18:56.527422Z",
     "shell.execute_reply": "2024-05-29T15:18:56.564856Z"
    },
    "trusted": true
   },
   "execution_count": 12,
   "outputs": [
    {
     "execution_count": 12,
     "output_type": "execute_result",
     "data": {
      "text/plain": "     idx                                           sentence  label  \\\n0      0    it 's a charming and often affecting journey .       1   \n1      1                 unflinchingly bleak and desperate       0   \n2      2  allows us to hope that nolan is poised to emba...      1   \n3      3  the acting , costumes , music , cinematography...      1   \n4      4                  it 's slow -- very , very slow .       0   \n..   ...                                                ...    ...   \n867  867              has all the depth of a wading pool .       0   \n868  868              a movie with a real anarchic flair .       1   \n869  869  a subject like this should inspire reaction in...      0   \n870  870  ... is an arthritic attempt at directing by ca...      0   \n871  871  looking aristocratic , luminous yet careworn i...      1   \n\n                                         text_wo_punct  \\\n0        it s a charming and often affecting journey     \n1                   unflinchingly bleak and desperate    \n2    allows us to hope that nolan is poised to emba...   \n3    the acting  costumes  music  cinematography an...   \n4                         it s slow  very  very slow     \n..                                                 ...   \n867               has all the depth of a wading pool     \n868               a movie with a real anarchic flair     \n869  a subject like this should inspire reaction in...   \n870   is an arthritic attempt at directing by calli...   \n871  looking aristocratic  luminous yet careworn in...   \n\n                                          text_wo_stop  \\\n0                     charming often affecting journey   \n1                        unflinchingly bleak desperate   \n2    allows us hope nolan poised embark major caree...   \n3    acting costumes music cinematography sound ast...   \n4                                            slow slow   \n..                                                 ...   \n867                                  depth wading pool   \n868                          movie real anarchic flair   \n869     subject like inspire reaction audience pianist   \n870          arthritic attempt directing callie khouri   \n871  looking aristocratic luminous yet careworn jan...   \n\n                                          text_stemmed  \\\n0                           charm often affect journey   \n1                           unflinchingli bleak desper   \n2    allow us hope nolan pois embark major career c...   \n3    act costum music cinematographi sound astound ...   \n4                                            slow slow   \n..                                                 ...   \n867                                    depth wade pool   \n868                             movi real anarch flair   \n869       subject like inspir reaction audienc pianist   \n870                arthrit attempt direct calli khouri   \n871  look aristocrat lumin yet careworn jane hamilt...   \n\n                                                tokens  \\\n0                      [charm, often, affect, journey]   \n1                       [unflinchingli, bleak, desper]   \n2    [allow, us, hope, nolan, pois, embark, major, ...   \n3    [act, costum, music, cinematographi, sound, as...   \n4                                         [slow, slow]   \n..                                                 ...   \n867                                [depth, wade, pool]   \n868                        [movi, real, anarch, flair]   \n869  [subject, like, inspir, reaction, audienc, pia...   \n870          [arthrit, attempt, direct, calli, khouri]   \n871  [look, aristocrat, lumin, yet, careworn, jane,...   \n\n                                        indexed_tokens  \\\n0                              [530, 2220, 2475, 1829]   \n1                                      [1112, 10, 333]   \n2    [343, 2803, 2841, 1977, 770, 2604, 1856, 2965,...   \n3    [1924, 2564, 215, 1859, 2470, 2892, 1024, 702,...   \n4                                           [747, 747]   \n..                                                 ...   \n867                                    [407, 905, 520]   \n868                             [848, 575, 1572, 2426]   \n869                  [370, 3375, 536, 1860, 1355, 257]   \n870                      [2123, 2566, 1550, 1684, 622]   \n871  [90, 1341, 552, 961, 1450, 182, 2356, 2802, 25...   \n\n                                         padded_tokens  \n0    [530, 2220, 2475, 1829, 0, 0, 0, 0, 0, 0, 0, 0...  \n1    [1112, 10, 333, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n2    [343, 2803, 2841, 1977, 770, 2604, 1856, 2965,...  \n3    [1924, 2564, 215, 1859, 2470, 2892, 1024, 702,...  \n4    [747, 747, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...  \n..                                                 ...  \n867  [407, 905, 520, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n868  [848, 575, 1572, 2426, 0, 0, 0, 0, 0, 0, 0, 0,...  \n869  [370, 3375, 536, 1860, 1355, 257, 0, 0, 0, 0, ...  \n870  [2123, 2566, 1550, 1684, 622, 0, 0, 0, 0, 0, 0...  \n871  [90, 1341, 552, 961, 1450, 182, 2356, 2802, 25...  \n\n[872 rows x 9 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>idx</th>\n      <th>sentence</th>\n      <th>label</th>\n      <th>text_wo_punct</th>\n      <th>text_wo_stop</th>\n      <th>text_stemmed</th>\n      <th>tokens</th>\n      <th>indexed_tokens</th>\n      <th>padded_tokens</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>it 's a charming and often affecting journey .</td>\n      <td>1</td>\n      <td>it s a charming and often affecting journey</td>\n      <td>charming often affecting journey</td>\n      <td>charm often affect journey</td>\n      <td>[charm, often, affect, journey]</td>\n      <td>[530, 2220, 2475, 1829]</td>\n      <td>[530, 2220, 2475, 1829, 0, 0, 0, 0, 0, 0, 0, 0...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>unflinchingly bleak and desperate</td>\n      <td>0</td>\n      <td>unflinchingly bleak and desperate</td>\n      <td>unflinchingly bleak desperate</td>\n      <td>unflinchingli bleak desper</td>\n      <td>[unflinchingli, bleak, desper]</td>\n      <td>[1112, 10, 333]</td>\n      <td>[1112, 10, 333, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>allows us to hope that nolan is poised to emba...</td>\n      <td>1</td>\n      <td>allows us to hope that nolan is poised to emba...</td>\n      <td>allows us hope nolan poised embark major caree...</td>\n      <td>allow us hope nolan pois embark major career c...</td>\n      <td>[allow, us, hope, nolan, pois, embark, major, ...</td>\n      <td>[343, 2803, 2841, 1977, 770, 2604, 1856, 2965,...</td>\n      <td>[343, 2803, 2841, 1977, 770, 2604, 1856, 2965,...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>the acting , costumes , music , cinematography...</td>\n      <td>1</td>\n      <td>the acting  costumes  music  cinematography an...</td>\n      <td>acting costumes music cinematography sound ast...</td>\n      <td>act costum music cinematographi sound astound ...</td>\n      <td>[act, costum, music, cinematographi, sound, as...</td>\n      <td>[1924, 2564, 215, 1859, 2470, 2892, 1024, 702,...</td>\n      <td>[1924, 2564, 215, 1859, 2470, 2892, 1024, 702,...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>it 's slow -- very , very slow .</td>\n      <td>0</td>\n      <td>it s slow  very  very slow</td>\n      <td>slow slow</td>\n      <td>slow slow</td>\n      <td>[slow, slow]</td>\n      <td>[747, 747]</td>\n      <td>[747, 747, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>867</th>\n      <td>867</td>\n      <td>has all the depth of a wading pool .</td>\n      <td>0</td>\n      <td>has all the depth of a wading pool</td>\n      <td>depth wading pool</td>\n      <td>depth wade pool</td>\n      <td>[depth, wade, pool]</td>\n      <td>[407, 905, 520]</td>\n      <td>[407, 905, 520, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n    </tr>\n    <tr>\n      <th>868</th>\n      <td>868</td>\n      <td>a movie with a real anarchic flair .</td>\n      <td>1</td>\n      <td>a movie with a real anarchic flair</td>\n      <td>movie real anarchic flair</td>\n      <td>movi real anarch flair</td>\n      <td>[movi, real, anarch, flair]</td>\n      <td>[848, 575, 1572, 2426]</td>\n      <td>[848, 575, 1572, 2426, 0, 0, 0, 0, 0, 0, 0, 0,...</td>\n    </tr>\n    <tr>\n      <th>869</th>\n      <td>869</td>\n      <td>a subject like this should inspire reaction in...</td>\n      <td>0</td>\n      <td>a subject like this should inspire reaction in...</td>\n      <td>subject like inspire reaction audience pianist</td>\n      <td>subject like inspir reaction audienc pianist</td>\n      <td>[subject, like, inspir, reaction, audienc, pia...</td>\n      <td>[370, 3375, 536, 1860, 1355, 257]</td>\n      <td>[370, 3375, 536, 1860, 1355, 257, 0, 0, 0, 0, ...</td>\n    </tr>\n    <tr>\n      <th>870</th>\n      <td>870</td>\n      <td>... is an arthritic attempt at directing by ca...</td>\n      <td>0</td>\n      <td>is an arthritic attempt at directing by calli...</td>\n      <td>arthritic attempt directing callie khouri</td>\n      <td>arthrit attempt direct calli khouri</td>\n      <td>[arthrit, attempt, direct, calli, khouri]</td>\n      <td>[2123, 2566, 1550, 1684, 622]</td>\n      <td>[2123, 2566, 1550, 1684, 622, 0, 0, 0, 0, 0, 0...</td>\n    </tr>\n    <tr>\n      <th>871</th>\n      <td>871</td>\n      <td>looking aristocratic , luminous yet careworn i...</td>\n      <td>1</td>\n      <td>looking aristocratic  luminous yet careworn in...</td>\n      <td>looking aristocratic luminous yet careworn jan...</td>\n      <td>look aristocrat lumin yet careworn jane hamilt...</td>\n      <td>[look, aristocrat, lumin, yet, careworn, jane,...</td>\n      <td>[90, 1341, 552, 961, 1450, 182, 2356, 2802, 25...</td>\n      <td>[90, 1341, 552, 961, 1450, 182, 2356, 2802, 25...</td>\n    </tr>\n  </tbody>\n</table>\n<p>872 rows × 9 columns</p>\n</div>"
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "\n",
    "# Tokenize sentences\n",
    "nltk.download('punkt')\n",
    "\n",
    "test_df['tokens'] = test_df['text_stemmed'].apply(lambda x: nltk.word_tokenize(x))\n",
    "\n",
    "# Create a word index (manual embedding)\n",
    "word_set = set()\n",
    "for tokens in test_df['tokens']:\n",
    "    word_set.update(tokens)\n",
    "\n",
    "word_index = {word: i+1 for i, word in enumerate(word_set)}  # Start indexing from 1\n",
    "\n",
    "# Convert tokens to integers\n",
    "test_df['indexed_tokens'] = test_df['tokens'].apply(lambda x: [word_index[token] for token in x])\n",
    "\n",
    "# Optional: Pad sequences to ensure uniform length\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "max_length = 100  # Define maximum length of sequences\n",
    "test_df['padded_tokens'] = pad_sequences(test_df['indexed_tokens'], maxlen=max_length, padding='post').tolist()"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-05-29T15:18:56.567496Z",
     "iopub.execute_input": "2024-05-29T15:18:56.567883Z",
     "iopub.status.idle": "2024-05-29T15:19:52.312916Z",
     "shell.execute_reply.started": "2024-05-29T15:18:56.567850Z",
     "shell.execute_reply": "2024-05-29T15:19:52.311886Z"
    },
    "trusted": true
   },
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "text": "[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "test_df.head()"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-05-29T15:19:52.314450Z",
     "iopub.execute_input": "2024-05-29T15:19:52.314829Z",
     "iopub.status.idle": "2024-05-29T15:19:52.347563Z",
     "shell.execute_reply.started": "2024-05-29T15:19:52.314797Z",
     "shell.execute_reply": "2024-05-29T15:19:52.346650Z"
    },
    "trusted": true
   },
   "execution_count": 14,
   "outputs": [
    {
     "execution_count": 14,
     "output_type": "execute_result",
     "data": {
      "text/plain": "                                            sentence  label  \\\n0  one of the other reviewers has mentioned that ...      1   \n1  a wonderful little production. <br /><br />the...      1   \n2  i thought this was a wonderful way to spend ti...      1   \n3  basically there's a family where a little boy ...      0   \n4  petter mattei's \"love in the time of money\" is...      1   \n\n                                       text_wo_punct  \\\n0  one of the other reviewers has mentioned that ...   \n1  a wonderful little production br br the filmin...   \n2  i thought this was a wonderful way to spend ti...   \n3  basically theres a family where a little boy j...   \n4  petter matteis love in the time of money is a ...   \n\n                                        text_wo_stop  \\\n0  one reviewers mentioned watching 1 oz episode ...   \n1  wonderful little production br br filming tech...   \n2  thought wonderful way spend time hot summer we...   \n3  basically theres family little boy jake thinks...   \n4  petter matteis love time money visually stunni...   \n\n                                        text_stemmed  \\\n0  one review mention watch 1 oz episod youll hoo...   \n1  wonder littl product br br film techniqu unass...   \n2  thought wonder way spend time hot summer weeke...   \n3  basic there famili littl boy jake think there ...   \n4  petter mattei love time money visual stun film...   \n\n                                              tokens  \\\n0  [one, review, mention, watch, 1, oz, episod, y...   \n1  [wonder, littl, product, br, br, film, techniq...   \n2  [thought, wonder, way, spend, time, hot, summe...   \n3  [basic, there, famili, littl, boy, jake, think...   \n4  [petter, mattei, love, time, money, visual, st...   \n\n                                      indexed_tokens  \\\n0  [40808, 108034, 54983, 140673, 125030, 113018,...   \n1  [13202, 68820, 103460, 87348, 87348, 104296, 2...   \n2  [41312, 13202, 134697, 33439, 62867, 111511, 6...   \n3  [35956, 141474, 97235, 68820, 37102, 80826, 30...   \n4  [137493, 38676, 38617, 62867, 94959, 77598, 73...   \n\n                                       padded_tokens  \n0  [39483, 59454, 115822, 31880, 74736, 118890, 5...  \n1  [13202, 68820, 103460, 87348, 87348, 104296, 2...  \n2  [41312, 13202, 134697, 33439, 62867, 111511, 6...  \n3  [35956, 141474, 97235, 68820, 37102, 80826, 30...  \n4  [59242, 132649, 33133, 131046, 91807, 33380, 1...  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>sentence</th>\n      <th>label</th>\n      <th>text_wo_punct</th>\n      <th>text_wo_stop</th>\n      <th>text_stemmed</th>\n      <th>tokens</th>\n      <th>indexed_tokens</th>\n      <th>padded_tokens</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>one of the other reviewers has mentioned that ...</td>\n      <td>1</td>\n      <td>one of the other reviewers has mentioned that ...</td>\n      <td>one reviewers mentioned watching 1 oz episode ...</td>\n      <td>one review mention watch 1 oz episod youll hoo...</td>\n      <td>[one, review, mention, watch, 1, oz, episod, y...</td>\n      <td>[40808, 108034, 54983, 140673, 125030, 113018,...</td>\n      <td>[39483, 59454, 115822, 31880, 74736, 118890, 5...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>a wonderful little production. &lt;br /&gt;&lt;br /&gt;the...</td>\n      <td>1</td>\n      <td>a wonderful little production br br the filmin...</td>\n      <td>wonderful little production br br filming tech...</td>\n      <td>wonder littl product br br film techniqu unass...</td>\n      <td>[wonder, littl, product, br, br, film, techniq...</td>\n      <td>[13202, 68820, 103460, 87348, 87348, 104296, 2...</td>\n      <td>[13202, 68820, 103460, 87348, 87348, 104296, 2...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>i thought this was a wonderful way to spend ti...</td>\n      <td>1</td>\n      <td>i thought this was a wonderful way to spend ti...</td>\n      <td>thought wonderful way spend time hot summer we...</td>\n      <td>thought wonder way spend time hot summer weeke...</td>\n      <td>[thought, wonder, way, spend, time, hot, summe...</td>\n      <td>[41312, 13202, 134697, 33439, 62867, 111511, 6...</td>\n      <td>[41312, 13202, 134697, 33439, 62867, 111511, 6...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>basically there's a family where a little boy ...</td>\n      <td>0</td>\n      <td>basically theres a family where a little boy j...</td>\n      <td>basically theres family little boy jake thinks...</td>\n      <td>basic there famili littl boy jake think there ...</td>\n      <td>[basic, there, famili, littl, boy, jake, think...</td>\n      <td>[35956, 141474, 97235, 68820, 37102, 80826, 30...</td>\n      <td>[35956, 141474, 97235, 68820, 37102, 80826, 30...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>petter mattei's \"love in the time of money\" is...</td>\n      <td>1</td>\n      <td>petter matteis love in the time of money is a ...</td>\n      <td>petter matteis love time money visually stunni...</td>\n      <td>petter mattei love time money visual stun film...</td>\n      <td>[petter, mattei, love, time, money, visual, st...</td>\n      <td>[137493, 38676, 38617, 62867, 94959, 77598, 73...</td>\n      <td>[59242, 132649, 33133, 131046, 91807, 33380, 1...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "\n",
    "param_grid = {\n",
    "    'tfidf__max_df': [0.95],\n",
    "    'tfidf__ngram_range': [(1, 3)],\n",
    "    'rf__n_estimators': [200],\n",
    "    'rf__max_depth': [None],\n",
    "    'rf__criterion': ['entropy'],\n",
    "    'rf__min_samples_leaf': [2]\n",
    "}\n",
    "\n",
    "\n",
    "X_train = train_df['text_wo_stop']\n",
    "y_train = train_df['label']\n",
    "X_valid = validation_df['text_wo_stop']\n",
    "y_valid = validation_df['label']\n",
    "\n",
    "\n",
    "# Create the pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer()),\n",
    "    ('rf', RandomForestClassifier(random_state=42))\n",
    "])\n",
    "\n",
    "# Perform grid search with cross-validation\n",
    "grid_search = GridSearchCV(pipeline, param_grid, cv=2, scoring='accuracy', n_jobs=-1, verbose=2)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Best parameters and estimator\n",
    "print(\"Best parameters found: \", grid_search.best_params_)\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Evaluate on validation set\n",
    "y_valid_pred = best_model.predict(X_valid)\n",
    "valid_accuracy = accuracy_score(y_valid, y_valid_pred)\n",
    "valid_report = classification_report(y_valid, y_valid_pred)\n",
    "\n",
    "print(f\"Validation Accuracy: {valid_accuracy}\")\n",
    "print(\"Validation Classification Report:\")\n",
    "print(valid_report)\n"
   ],
   "metadata": {
    "id": "64gFRM_7FaAV",
    "execution": {
     "iopub.status.busy": "2024-05-29T16:01:25.013952Z",
     "iopub.execute_input": "2024-05-29T16:01:25.014733Z",
     "iopub.status.idle": "2024-05-29T16:07:44.363219Z",
     "shell.execute_reply.started": "2024-05-29T16:01:25.014701Z",
     "shell.execute_reply": "2024-05-29T16:07:44.361948Z"
    },
    "trusted": true
   },
   "execution_count": 20,
   "outputs": [
    {
     "name": "stdout",
     "text": "Fitting 2 folds for each of 1 candidates, totalling 2 fits\nBest parameters found:  {'rf__criterion': 'entropy', 'rf__max_depth': None, 'rf__min_samples_leaf': 2, 'rf__n_estimators': 200, 'tfidf__max_df': 0.95, 'tfidf__ngram_range': (1, 3)}\nValidation Accuracy: 0.7786697247706422\nValidation Classification Report:\n              precision    recall  f1-score   support\n\n           0       0.81      0.71      0.76       428\n           1       0.75      0.84      0.79       444\n\n    accuracy                           0.78       872\n   macro avg       0.78      0.78      0.78       872\nweighted avg       0.78      0.78      0.78       872\n\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "X_test = test_df['text_wo_stop']\n",
    "y_test = test_df['label']\n",
    "\n",
    "y_test_pred = best_model.predict(X_test)\n",
    "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "test_report = classification_report(y_test, y_test_pred)\n",
    "\n",
    "print(f\"Test Accuracy: {test_accuracy}\")\n",
    "print(\"Test Classification Report:\")\n",
    "print(test_report)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EFGM-kWtH5fe",
    "outputId": "9dfc8e4e-6fa7-4d28-df66-73f7c2299ad7",
    "execution": {
     "iopub.status.busy": "2024-05-29T16:08:08.648674Z",
     "iopub.execute_input": "2024-05-29T16:08:08.648965Z",
     "iopub.status.idle": "2024-05-29T16:08:32.856544Z",
     "shell.execute_reply.started": "2024-05-29T16:08:08.648940Z",
     "shell.execute_reply": "2024-05-29T16:08:32.855550Z"
    },
    "trusted": true
   },
   "execution_count": 22,
   "outputs": [
    {
     "name": "stdout",
     "text": "[CV] END rf__criterion=entropy, rf__max_depth=None, rf__min_samples_leaf=2, rf__n_estimators=200, tfidf__max_df=0.95, tfidf__ngram_range=(1, 3); total time= 1.9min\n[CV] END rf__criterion=entropy, rf__max_depth=None, rf__min_samples_leaf=2, rf__n_estimators=200, tfidf__max_df=0.95, tfidf__ngram_range=(1, 3); total time= 1.9min\nTest Accuracy: 0.79592\nTest Classification Report:\n              precision    recall  f1-score   support\n\n           0       0.76      0.86      0.81     25000\n           1       0.84      0.73      0.78     25000\n\n    accuracy                           0.80     50000\n   macro avg       0.80      0.80      0.80     50000\nweighted avg       0.80      0.80      0.80     50000\n\n",
     "output_type": "stream"
    }
   ]
  }
 ]
}
